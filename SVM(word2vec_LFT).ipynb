{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('word2vec.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('656_dataset_fin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MFT_data=pd.read_csv('MFT_dataset.csv')\n",
    "LFT_data=pd.read_csv('LFT_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_ID</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>fine_grain</th>\n",
       "      <th>sentence</th>\n",
       "      <th>coarse_grain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>European Union</td>\n",
       "      <td>['/people/ethnicity', '/location/country', '/l...</td>\n",
       "      <td>112 Emergency phone number across the European...</td>\n",
       "      <td>/people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>European Union</td>\n",
       "      <td>['/people/ethnicity', '/location/country', '/l...</td>\n",
       "      <td>112 Emergency phone number across the European...</td>\n",
       "      <td>/location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>European Union</td>\n",
       "      <td>['/people/ethnicity', '/location/country', '/l...</td>\n",
       "      <td>112 Emergency phone number across the European...</td>\n",
       "      <td>/person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>Stockton and Darlington Railway</td>\n",
       "      <td>['/rail/railway', '/organization', '/organizat...</td>\n",
       "      <td>The first 0-4-0 to use coupling rods was Locom...</td>\n",
       "      <td>/rail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>Stockton and Darlington Railway</td>\n",
       "      <td>['/rail/railway', '/organization', '/organizat...</td>\n",
       "      <td>The first 0-4-0 to use coupling rods was Locom...</td>\n",
       "      <td>/organization</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentence_ID                      entity_name  \\\n",
       "0           0           15                   European Union   \n",
       "1           1           15                   European Union   \n",
       "2           2           15                   European Union   \n",
       "3           3           44  Stockton and Darlington Railway   \n",
       "4           4           44  Stockton and Darlington Railway   \n",
       "\n",
       "                                          fine_grain  \\\n",
       "0  ['/people/ethnicity', '/location/country', '/l...   \n",
       "1  ['/people/ethnicity', '/location/country', '/l...   \n",
       "2  ['/people/ethnicity', '/location/country', '/l...   \n",
       "3  ['/rail/railway', '/organization', '/organizat...   \n",
       "4  ['/rail/railway', '/organization', '/organizat...   \n",
       "\n",
       "                                            sentence   coarse_grain  \n",
       "0  112 Emergency phone number across the European...        /people  \n",
       "1  112 Emergency phone number across the European...      /location  \n",
       "2  112 Emergency phone number across the European...        /person  \n",
       "3  The first 0-4-0 to use coupling rods was Locom...          /rail  \n",
       "4  The first 0-4-0 to use coupling rods was Locom...  /organization  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset):\n",
    "    \n",
    "    X, y = np.asarray(dataset['sentence']), np.asarray(dataset['coarse_grain'])\n",
    "    label_map = {cat:index for index,cat in enumerate(np.unique(y))}\n",
    "    y_prep = np.asarray([label_map[l] for l in y])\n",
    "    x_tokenized = [[w for w in sentence.split(\" \") if w != \"\"] for sentence in X]\n",
    "   \n",
    "    return x_tokenized, y_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequencer():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 all_words,\n",
    "                 max_words,\n",
    "                 seq_len,\n",
    "                 embedding_matrix\n",
    "                ):\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.embed_matrix = embedding_matrix\n",
    "        \"\"\"\n",
    "        temp_vocab = Vocab which has all the unique words\n",
    "        self.vocab = Our last vocab which has only most used N words.\n",
    "    \n",
    "        \"\"\"\n",
    "        temp_vocab = list(set(all_words))\n",
    "        self.vocab = []\n",
    "        self.word_cnts = {}\n",
    "        \"\"\"\n",
    "        Now we'll create a hash map (dict) which includes words and their occurencies\n",
    "        \"\"\"\n",
    "        for word in temp_vocab:\n",
    "            # 0 does not have a meaning, you can add the word to the list\n",
    "            # or something different.\n",
    "            count = len([0 for w in all_words if w == word])\n",
    "            self.word_cnts[word] = count\n",
    "            counts = list(self.word_cnts.values())\n",
    "            indexes = list(range(len(counts)))\n",
    "        \n",
    "        # Now we'll sort counts and while sorting them also will sort indexes.\n",
    "        # We'll use those indexes to find most used N word.\n",
    "        cnt = 0\n",
    "        while cnt + 1 != len(counts):\n",
    "            cnt = 0\n",
    "            for i in range(len(counts)-1):\n",
    "                if counts[i] < counts[i+1]:\n",
    "                    counts[i+1],counts[i] = counts[i],counts[i+1]\n",
    "                    indexes[i],indexes[i+1] = indexes[i+1],indexes[i]\n",
    "                else:\n",
    "                    cnt += 1\n",
    "        \n",
    "        for ind in indexes[:max_words]:\n",
    "            self.vocab.append(temp_vocab[ind])\n",
    "                    \n",
    "    def textToVector(self,text):\n",
    "        # First we need to split the text into its tokens and learn the length\n",
    "        # If length is shorter than the max len we'll add some spaces (100D vectors which has only zero values)\n",
    "        # If it's longer than the max len we'll trim from the end.\n",
    "        tokens = text.split()\n",
    "        len_v = len(tokens)-1 if len(tokens) < self.seq_len else self.seq_len-1\n",
    "        vec = []\n",
    "        for tok in tokens[:len_v]:\n",
    "            try:\n",
    "                vec.append(self.embed_matrix[tok])\n",
    "            except Exception as E:\n",
    "                pass\n",
    "        \n",
    "        last_pieces = self.seq_len - len(vec)\n",
    "        for i in range(last_pieces):\n",
    "            vec.append(np.zeros(100,))\n",
    "        \n",
    "        return np.asarray(vec).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. For whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenized, y_prep = preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Textory/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "sequencer = Sequencer(all_words = [token for seq in x_tokenized for token in seq],\n",
    "              max_words = 1200,\n",
    "              seq_len = 15,\n",
    "              embedding_matrix = model.wv\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in x_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_model = PCA(n_components=50)\n",
    "pca_model.fit(x_vecs)\n",
    "print(\"Sum of variance ratios: \",sum(pca_model.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comps = pca_model.transform(x_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x_comps,y_prep,test_size=0.2,random_state=42)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. For MFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenized, y_prep = preprocessing(MFT_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencer = Sequencer(all_words = [token for seq in x_tokenized for token in seq],\n",
    "              max_words = 1200,\n",
    "              seq_len = 15,\n",
    "              embedding_matrix = model.wv\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in x_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_model = PCA(n_components=50)\n",
    "pca_model.fit(x_vecs)\n",
    "print(\"Sum of variance ratios: \",sum(pca_model.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comps = pca_model.transform(x_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x_comps,y_prep,test_size=0.2,random_state=42)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. For LFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec embeddings for the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenized, y_prep = preprocessing(LFT_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Textory/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "sequencer = Sequencer(all_words = [token for seq in x_tokenized for token in seq],\n",
    "              max_words = 120,\n",
    "              seq_len = 15,\n",
    "              embedding_matrix = model.wv\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in x_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of variance ratios:  0.3631605459465171\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_model = PCA(n_components=50)\n",
    "pca_model.fit(x_vecs)\n",
    "print(\"Sum of variance ratios: \",sum(pca_model.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comps = pca_model.transform(x_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec embeddings for the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entities = np.asarray(LFT_data['entity_name'])\n",
    "e_tokenized = [[w for w in e.split(\" \") if w != \"\"] for e in Entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Textory/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "entity_sequencer = Sequencer(all_words = [token for seq in e_tokenized for token in seq],\n",
    "              max_words = 10,\n",
    "              seq_len = 5,\n",
    "              embedding_matrix = model.wv\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_vecs = np.asarray([entity_sequencer.textToVector(\" \".join(seq)) for seq in e_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of variance ratios:  0.9187674098513777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_model = PCA(n_components=50)\n",
    "pca_model.fit(e_vecs)\n",
    "print(\"Sum of variance ratios: \",sum(pca_model.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_comps = pca_model.transform(e_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenate sentence embeddings + entity embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total = np.hstack([e_comps,x_comps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82147, 100)\n",
      "(20537, 100)\n",
      "(82147,)\n",
      "(20537,)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(e_comps,y_prep,test_size=0.2,random_state=42)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1741247504504066"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
